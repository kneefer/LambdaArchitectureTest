sparkMaster = "local[*]"
hdfsDataPath = "hdfs://localhost:9000/spark/data"
streamingBatchDurationSeconds = 10
streamingWindowDurationSeconds = 30
checkpointDir = "hdfs://localhost:9000/spark/checkpoints"
hllBitsCount = 12
batchBucketMinutes = 60

cassandra {
  port = 9042
  host = "127.0.0.1"
  keepAliveMs = 15000
  userName = "cassandra"
  keyspaceName = "lambda"
  tables = [
    "batch_unique_visitors_by_site",
    "batch_actions_by_site",
    "speed_unique_visitors_by_site",
    "speed_actions_by_site"
  ]
}

postgres {
  connectionString = "jdbc:postgresql://localhost:5432/lambda?user=postgres&password=mysecretpassword"
}

kafka {
  topic = "lambda"
  numOfPartitions = 1

  producer {
    clientId = "LambdaDataProducer"
    bootstrapServers = "localhost:9092"
    acks = "all"
    maxRetries = 3
    batchSizeBytes = 1638
    lingerTimeMs = 1
    bufferSizeBytes = 33554432
    keySerializerClass = "org.apache.kafka.common.serialization.StringSerializer"
    valueSerializerClass = "org.apache.kafka.common.serialization.StringSerializer"
  }

  streamConsumer {
    clientId = "HdfsConsumer"
    groupId = "hdfs_consumer"
    zookeeperConnect = "localhost:2181"
    enableAutoCommit = "true"
    autoOffsetReset = "largest"
    consumerTimeoutMs = 500
    autoCommitIntervalMs = 1000
    keyDeserializerClass = "org.apache.kafka.common.serialization.StringDeserializer"
    valueDeserializerClass = "org.apache.kafka.common.serialization.StringDeserializer"
  }
}

trafficGen {
  recordsPerBatch = 30000
  numOfBatches = 1000
  numOfVisitors = 1000000
  timeMultiplier = 1
  numOfSubpages = 15
  sleepAfterEachFileMs = 2000
}

