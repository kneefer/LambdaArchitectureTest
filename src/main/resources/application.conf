sparkMaster = "local[*]"
hdfsDataPath = "hdfs://localhost:9000/spark/data"
hdfsBatchImagesPath = "hdfs://localhost:9000/spark/batch_imgs"
streamingBatchDurationSeconds = 10
streamingWindowDurationSeconds = 30
checkpointDir = "hdfs://localhost:9000/spark/checkpoints"
hllBitsCount = 12

cassandra {
  port = 9042
  host = "127.0.0.1"
  userName = "cassandra"
  keyspaceName = "lambda"
  tables = [
    "batch_unique_visitors_by_site",
    "batch_actions_by_site",
    "speed_unique_visitors_by_site",
    "speed_actions_by_site"
  ]
}

kafka {
  topic = "lambda"
  numOfPartitions = 1

  producer {
    clientId = "LambdaDataProducer"
    bootstrapServers = "localhost:9092"
    acks = "all"
    maxRetries = 3
    batchSizeBytes = 1638
    lingerTimeMs = 1
    bufferSizeBytes = 33554432
    keySerializerClass = "org.apache.kafka.common.serialization.StringSerializer"
    valueSerializerClass = "org.apache.kafka.common.serialization.StringSerializer"
  }

  streamConsumer {
    clientId = "HdfsConsumer"
    groupId = "hdfs_consumer"
    zookeeperConnect = "localhost:2181"
    enableAutoCommit = "true"
    autoOffsetReset = "largest"
    consumerTimeoutMs = 500
    autoCommitIntervalMs = 1000
    keyDeserializerClass = "org.apache.kafka.common.serialization.StringDeserializer"
    valueDeserializerClass = "org.apache.kafka.common.serialization.StringDeserializer"
  }
}

trafficGen {
  recordsPerBatch = 10000
  numOfBatches = 1000
  numOfVisitors = 1000000
  timeMultiplier = 1
  numOfSubpages = 15
  sleepAfterEachFileMs = 2000
}

