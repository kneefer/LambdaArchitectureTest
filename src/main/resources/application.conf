sparkMaster = "local[*]"
hdfsDataPath = "hdfs://localhost:9000/spark/data"
hdfsBatchImagesPath = "hdfs://localhost:9000/spark/batch_imgs"
streamingBatchDurationSeconds = 5
checkpointDir = "hdfs://localhost:9000/spark/checkpoints"

cassandra {
  port = 9042
  hosts = ["127.0.0.1"]
  writeConsistency = "ONE"
  readConsistency = "ONE"
  replicationFactor = 1
}

kafka {
  topic = "lambda"
  numOfPartitions = 1

  producer {
    clientId = "LambdaDataProducer"
    bootstrapServers = "localhost:9092"
    acks = "all"
    maxRetries = 3
    batchSizeBytes = 1638
    lingerTimeMs = 1
    bufferSizeBytes = 33554432
    keySerializerClass = "org.apache.kafka.common.serialization.StringSerializer"
    valueSerializerClass = "org.apache.kafka.common.serialization.StringSerializer"
  }

  hdfsConsumer {
    clientId = "HdfsConsumer"
    groupId = "hdfs_consumer"
    zookeeperConnect = "localhost:2181"
    enableAutoCommit = "true"
    autoOffsetReset = "largest"
    consumerTimeoutMs = 500
    autoCommitIntervalMs = 1000
    keyDeserializerClass = "org.apache.kafka.common.serialization.StringDeserializer"
    valueDeserializerClass = "org.apache.kafka.common.serialization.StringDeserializer"
  }

  batchConsumer {
    clientId = "BatchConsumer"
    groupId = "batch_consumer"
    zookeeperConnect = "localhost:2181"
    enableAutoCommit = "true"
    autoOffsetReset = "smallest"
    consumerTimeoutMs = 500
    autoCommitIntervalMs = 1000
    keyDeserializerClass = "org.apache.kafka.common.serialization.StringDeserializer"
    valueDeserializerClass = "org.apache.kafka.common.serialization.StringDeserializer"
  }
}

trafficGen {
  recordsPerBatch = 10000
  numOfBatches = 1000
  numOfVisitors = 1000000
  timeMultiplier = 1
  numOfSubpages = 15
  sleepAfterEachFileMs = 2000
}

